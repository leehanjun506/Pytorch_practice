{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfd02dc",
   "metadata": {},
   "source": [
    "# 선형 회귀 (Linear Regression)\n",
    "---\n",
    "\n",
    "\n",
    "## 1. 선형 회귀 (Linear Regression)\n",
    "### 1. 데이터에 대한 이해(Data Definition)\n",
    "1. 훈련 데이터셋과 테스트 데이터셋  \n",
    "훈련 데이터셋(training dataset) : 예측을 위해 사용하는 데이터 학습이 끝난 후, 모델이 얼마나 잘 작동하는지 판별하는 데이터셋\n",
    "2. 훈련 데이터셋의 구성  \n",
    "모델을 학습시키기 위해 텐서의 형태(torch.tensor)를 가지고 있는다.\n",
    "\n",
    "### 2. 가설(Hypothesis) 수립\n",
    "선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다.  \n",
    "이때 선형 회귀의 가설은 H(x) = Wx + b 와 같은 형식을 가지고 W를 가중치(Weight), b를 편향(bias)라고 한다.\n",
    "\n",
    "### 3. 비용 함수(Cost function)에 대한 이해\n",
    "비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)  \n",
    "평균 제곱 오차(MSE)는 회귀 문제에서 적절한 가중치와 편향을 찾기 위해서 최적화된 식이다.  \n",
    "평균 제곱 오차의 값을 최소값으로 만드는 가중치와 편향을 찾아내는 것이 가장 훈련 데이터를 잘 반영한 직선을 찾아내는 일이기 때문.\n",
    "\n",
    "### 4. 옵티마이저  - 경사 하강법(Gradient Descent)\n",
    "Cost function의 값을 최소로 하는 가중치와 편향을 찾을 때 옵티마이저(Optimizer) 알고리즘을 사용한다. 이때 사용하는 알고리즘 중 하나가 경사 하강법(Gradient Descent)이다.  \n",
    "이때 가중치의 값을 얼마나 변경할 지를 위한 적절한 학습률(learning rate)의 값을 찾아내는 것도 중요하다.  \n",
    "\n",
    "### 5. 파이토치로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f86e21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.1867, b: 0.0800, Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.7457, b: 0.5781, Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.8001, b: 0.4544, Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.8429, b: 0.3572, Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.8765, b: 0.2808, Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.9029, b: 0.2207, Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.9237, b: 0.1735, Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.9400, b: 0.1364, Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.9528, b: 0.1072, Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.9629, b: 0.0843, Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.9709, b: 0.0663, Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.9771, b: 0.0521, Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.9820, b: 0.0409, Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.9858, b: 0.0322, Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.9889, b: 0.0253, Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.9913, b: 0.0199, Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.9931, b: 0.0156, Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.9946, b: 0.0123, Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.9958, b: 0.0097, Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.9967, b: 0.0076, Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.9974, b: 0.0060, Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1) # 재실행해도 같은 결과가 나오도록 랜덤 시드를 준다.\n",
    "\n",
    "x_train = torch.FloatTensor([[1],[2],[3]]) # 입력\n",
    "y_train = torch.FloatTensor([[2],[4],[6]]) # 출력\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True) # 가중치 0으로 초기화, 학습을 통해 값이 변경되는 변수 명시\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b],lr=0.01) # optimizer 설정\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    hypothesis = x_train*W + b # 가설 선언\n",
    "\n",
    "    cost = torch.mean((hypothesis  - y_train)**2) # 비용 함수에 해당되는 평균 제곱 오차 선언\n",
    "\n",
    "\n",
    "    optimizer.zero_grad() # gradient 0으로 초기화\n",
    "    cost.backward() # 역전파 사용하여 gradient 계산\n",
    "    optimizer.step() # W와 b 업데이트\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.4f}, b: {:.4f}, Cost: {:.6f}'.format(epoch,nb_epochs,W.item(),b.item(),cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84515412",
   "metadata": {},
   "source": [
    "W는 2에 가깝고, b는 0에 가깝다. 현재 훈련데이터에서 실제 정답은 W=2, b=0이므로 거의 정답을 찾았다.  \n",
    "### 6. optimizer.zero_grad() 필요 이유\n",
    "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시킨다. 따라서 미분값을 0으로 계속 초기화시켜줘야 한다.\n",
    "### 7. torch.manual_seed()를 하는 이유\n",
    "torch.manual_seed()는 동일한 시드에서 난수 발생 순서와 값을 보장해준다.\n",
    "\n",
    "## 2. 자동 미분(Autograd)\n",
    "### 1. 자동 미분 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5826b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "y = w**2\n",
    "z = 2*y+5\n",
    "\n",
    "z.backward() #해당 수식의 w에 대한 기울기 계산\n",
    "\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad)) # w가 속한 수식을 w로 미분한 값이 w.grad에 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ef411",
   "metadata": {},
   "source": [
    "## 3. 다중 선형 회귀(Multivariable Linear regression)\n",
    "다중 선형 회귀 : 다수의 입력으로부터 출력을 예측.\n",
    "### 1. 파이토치로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f72d3a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b:0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b:0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b:0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b:0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b:0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b:0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b:0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b:0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b:0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b:0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b:0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73],[93],[89],[96],[73]])\n",
    "x2_train = torch.FloatTensor([[80],[88],[91],[98],[66]])\n",
    "x3_train = torch.FloatTensor([[75],[93],[90],[100],[70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 가중치와 편향 초기화\n",
    "w1 = torch.zeros(1,requires_grad=True)\n",
    "w2 = torch.zeros(1,requires_grad=True)\n",
    "w3 = torch.zeros(1,requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1,w2,w3,b],lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b:{:.3f} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,w1.item(),w2.item(),w3.item(),b.item(),cost.item()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd4062",
   "metadata": {},
   "source": [
    "### 2. 벡터와 행렬 연산으로 바꾸기\n",
    "입력의 개수가 많아 질수록 일일이 선언하기 비효율적이므로 행렬 곱셈 연산(또는 벡터의 내적)을 사용한다.\n",
    "\n",
    "### 3. 행렬 연산을 고려하여 파이토치로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a3ff73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b],lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    # b는 broadcasting되어 더해진다.\n",
    "    hypothesis = x_train.matmul(W)+b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item() # detach() : 기존 Tensor에서 gradient 전파가 안되는 텐서 생성\n",
    "    )) #hypothesis 가 점점 y_train값에 가까워진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01cdea",
   "metadata": {},
   "source": [
    "## 4. nn.Module로 구현하는 선형 회귀\n",
    "### 1. 단순 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2deaa6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n",
      "Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)\n",
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "# 모델을 선언 및 초기화 \n",
    "model = nn.Linear(1,1) # 단순 선형 회귀이므로 입력 뉴런과 출력 뉴런은 1\n",
    "\n",
    "print(list(model.parameters())) # 첫 번째 값: W 두 번째 값 : b\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction,y_train) #평균제곱오차사용\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "         print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n",
    "            \n",
    "# 최적화가 되었는지 확인\n",
    "\n",
    "new_var = torch.FloatTensor([[4.0]])\n",
    "pred_y = model(new_var)\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\",pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64730e1",
   "metadata": {},
   "source": [
    "### 2. 다중 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c94e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 31667.599609\n",
      "Epoch  100/2000 Cost: 0.225993\n",
      "Epoch  200/2000 Cost: 0.223911\n",
      "Epoch  300/2000 Cost: 0.221941\n",
      "Epoch  400/2000 Cost: 0.220059\n",
      "Epoch  500/2000 Cost: 0.218271\n",
      "Epoch  600/2000 Cost: 0.216575\n",
      "Epoch  700/2000 Cost: 0.214950\n",
      "Epoch  800/2000 Cost: 0.213413\n",
      "Epoch  900/2000 Cost: 0.211952\n",
      "Epoch 1000/2000 Cost: 0.210559\n",
      "Epoch 1100/2000 Cost: 0.209230\n",
      "Epoch 1200/2000 Cost: 0.207967\n",
      "Epoch 1300/2000 Cost: 0.206762\n",
      "Epoch 1400/2000 Cost: 0.205618\n",
      "Epoch 1500/2000 Cost: 0.204529\n",
      "Epoch 1600/2000 Cost: 0.203481\n",
      "Epoch 1700/2000 Cost: 0.202486\n",
      "Epoch 1800/2000 Cost: 0.201539\n",
      "Epoch 1900/2000 Cost: 0.200634\n",
      "Epoch 2000/2000 Cost: 0.199770\n",
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2306]], grad_fn=<AddmmBackward>)\n",
      "훈련 후 w와b의 값 :  [Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train) \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n",
    "\n",
    "# 최적화 확인코드\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "pred_y = model(new_var) \n",
    "print('훈련 후 입력이 73, 80, 75일 때의 예측값 :', pred_y) \n",
    "\n",
    "print('훈련 후 w와b의 값 : ',list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70413379",
   "metadata": {},
   "source": [
    "## 5. 클래스로 파이토치 모델 구현하기\n",
    "### 1. 모델을 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd21f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 선형 회귀 모델 선언 및 초기화\n",
    "model1 = nn.Linear(1,1)\n",
    "\n",
    "# 이를 클래스로 구현\n",
    "class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 클래스\n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module 클래스의 속성들을 가지고 초기화.\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model1 = LinearRegressionModel()\n",
    "\n",
    "# 다중 선형 회귀 모델 선언 및 초기화\n",
    "model2 = nn.Linear(3,1)\n",
    "\n",
    "#이를 클래스로 구현\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model2 = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb218624",
   "metadata": {},
   "source": [
    "### 2. 단순 선형 회귀 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff08030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n",
      "tensor([[2.0008],\n",
      "        [4.0002],\n",
      "        [5.9995]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train) \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n",
    "print(model.forward(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496b9ba",
   "metadata": {},
   "source": [
    "### 3. 다중 선형 회귀 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb0e2ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 31667.599609\n",
      "Epoch  100/2000 Cost: 0.225993\n",
      "Epoch  200/2000 Cost: 0.223911\n",
      "Epoch  300/2000 Cost: 0.221941\n",
      "Epoch  400/2000 Cost: 0.220059\n",
      "Epoch  500/2000 Cost: 0.218271\n",
      "Epoch  600/2000 Cost: 0.216575\n",
      "Epoch  700/2000 Cost: 0.214950\n",
      "Epoch  800/2000 Cost: 0.213413\n",
      "Epoch  900/2000 Cost: 0.211952\n",
      "Epoch 1000/2000 Cost: 0.210559\n",
      "Epoch 1100/2000 Cost: 0.209230\n",
      "Epoch 1200/2000 Cost: 0.207967\n",
      "Epoch 1300/2000 Cost: 0.206762\n",
      "Epoch 1400/2000 Cost: 0.205618\n",
      "Epoch 1500/2000 Cost: 0.204529\n",
      "Epoch 1600/2000 Cost: 0.203481\n",
      "Epoch 1700/2000 Cost: 0.202486\n",
      "Epoch 1800/2000 Cost: 0.201539\n",
      "Epoch 1900/2000 Cost: 0.200634\n",
      "Epoch 2000/2000 Cost: 0.199770\n",
      "tensor([[151.2306],\n",
      "        [184.8005],\n",
      "        [180.5203],\n",
      "        [196.3101],\n",
      "        [141.9926]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train) \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n",
    "print(model.forward(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6f4d6",
   "metadata": {},
   "source": [
    "## 6. 미니 배치와 데이터 로드(Mini Batch and Data Load)\n",
    "### 1. 미니 배치와 배치 크기(Mini Batch and Batch Size)\n",
    "데이터가 수십만개 이상이라면 학습이 매우 느리고 많은 계산량이 필요하다.  \n",
    "때문에 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 방법이 필요.  \n",
    "* 에포크(Epoch)는 전체 훈련 데이터가 학습에 한 번 사용된 주기이다.\n",
    "* 미니 배치 경사 하강법은 전체 데이터의 일부만을 보고 수행하므로 최적값으로 수렴하는 과정에서 값이 조금 헤매기도 하지만 훈련 속도가 빠름."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22246f",
   "metadata": {},
   "source": [
    "### 2. 이터레이션(Iteration)\n",
    "이터레이션이란 한 번의 에포크 내에서 이루어지는 매개변수인 W와 b의 업데이트 횟수이다.\n",
    "### 3. 데이터 로드하기(Data Load)\n",
    "파이토치에서는 데이터를 다루는 유용한 도구로서 데이터셋(Dataset)과 데이터로더(DataLoader)를 제공한다.  \n",
    "이를 사용해 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3706a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 Batch 1/3 Cost: 42582.093750\n",
      "Epoch    0/200 Batch 2/3 Cost: 13945.656250\n",
      "Epoch    0/200 Batch 3/3 Cost: 2449.562012\n",
      "Epoch    1/200 Batch 1/3 Cost: 1853.046997\n",
      "Epoch    1/200 Batch 2/3 Cost: 198.623291\n",
      "Epoch    1/200 Batch 3/3 Cost: 172.716934\n",
      "Epoch    2/200 Batch 1/3 Cost: 14.427309\n",
      "Epoch    2/200 Batch 2/3 Cost: 44.751465\n",
      "Epoch    2/200 Batch 3/3 Cost: 30.967724\n",
      "Epoch    3/200 Batch 1/3 Cost: 5.962751\n",
      "Epoch    3/200 Batch 2/3 Cost: 9.439251\n",
      "Epoch    3/200 Batch 3/3 Cost: 17.353144\n",
      "Epoch    4/200 Batch 1/3 Cost: 12.244507\n",
      "Epoch    4/200 Batch 2/3 Cost: 12.010944\n",
      "Epoch    4/200 Batch 3/3 Cost: 1.500939\n",
      "Epoch    5/200 Batch 1/3 Cost: 19.044754\n",
      "Epoch    5/200 Batch 2/3 Cost: 14.529252\n",
      "Epoch    5/200 Batch 3/3 Cost: 0.601751\n",
      "Epoch    6/200 Batch 1/3 Cost: 20.386246\n",
      "Epoch    6/200 Batch 2/3 Cost: 13.803010\n",
      "Epoch    6/200 Batch 3/3 Cost: 0.501859\n",
      "Epoch    7/200 Batch 1/3 Cost: 0.388286\n",
      "Epoch    7/200 Batch 2/3 Cost: 13.472797\n",
      "Epoch    7/200 Batch 3/3 Cost: 20.895149\n",
      "Epoch    8/200 Batch 1/3 Cost: 12.472652\n",
      "Epoch    8/200 Batch 2/3 Cost: 9.304205\n",
      "Epoch    8/200 Batch 3/3 Cost: 4.664165\n",
      "Epoch    9/200 Batch 1/3 Cost: 1.388083\n",
      "Epoch    9/200 Batch 2/3 Cost: 20.627062\n",
      "Epoch    9/200 Batch 3/3 Cost: 19.485430\n",
      "Epoch   10/200 Batch 1/3 Cost: 15.217678\n",
      "Epoch   10/200 Batch 2/3 Cost: 7.506926\n",
      "Epoch   10/200 Batch 3/3 Cost: 3.931872\n",
      "Epoch   11/200 Batch 1/3 Cost: 15.117404\n",
      "Epoch   11/200 Batch 2/3 Cost: 7.870743\n",
      "Epoch   11/200 Batch 3/3 Cost: 2.163551\n",
      "Epoch   12/200 Batch 1/3 Cost: 7.878772\n",
      "Epoch   12/200 Batch 2/3 Cost: 14.613531\n",
      "Epoch   12/200 Batch 3/3 Cost: 3.639434\n",
      "Epoch   13/200 Batch 1/3 Cost: 0.589943\n",
      "Epoch   13/200 Batch 2/3 Cost: 22.074862\n",
      "Epoch   13/200 Batch 3/3 Cost: 18.487795\n",
      "Epoch   14/200 Batch 1/3 Cost: 4.003544\n",
      "Epoch   14/200 Batch 2/3 Cost: 13.883735\n",
      "Epoch   14/200 Batch 3/3 Cost: 15.112158\n",
      "Epoch   15/200 Batch 1/3 Cost: 12.698604\n",
      "Epoch   15/200 Batch 2/3 Cost: 8.300083\n",
      "Epoch   15/200 Batch 3/3 Cost: 6.982977\n",
      "Epoch   16/200 Batch 1/3 Cost: 16.262123\n",
      "Epoch   16/200 Batch 2/3 Cost: 8.561508\n",
      "Epoch   16/200 Batch 3/3 Cost: 11.661345\n",
      "Epoch   17/200 Batch 1/3 Cost: 0.091975\n",
      "Epoch   17/200 Batch 2/3 Cost: 14.031861\n",
      "Epoch   17/200 Batch 3/3 Cost: 21.752197\n",
      "Epoch   18/200 Batch 1/3 Cost: 13.175379\n",
      "Epoch   18/200 Batch 2/3 Cost: 10.374492\n",
      "Epoch   18/200 Batch 3/3 Cost: 10.069407\n",
      "Epoch   19/200 Batch 1/3 Cost: 10.599391\n",
      "Epoch   19/200 Batch 2/3 Cost: 11.538933\n",
      "Epoch   19/200 Batch 3/3 Cost: 15.889595\n",
      "Epoch   20/200 Batch 1/3 Cost: 9.525854\n",
      "Epoch   20/200 Batch 2/3 Cost: 5.581358\n",
      "Epoch   20/200 Batch 3/3 Cost: 19.525595\n",
      "Epoch   21/200 Batch 1/3 Cost: 8.570367\n",
      "Epoch   21/200 Batch 2/3 Cost: 14.372413\n",
      "Epoch   21/200 Batch 3/3 Cost: 1.803123\n",
      "Epoch   22/200 Batch 1/3 Cost: 10.986645\n",
      "Epoch   22/200 Batch 2/3 Cost: 11.502592\n",
      "Epoch   22/200 Batch 3/3 Cost: 1.802180\n",
      "Epoch   23/200 Batch 1/3 Cost: 8.313410\n",
      "Epoch   23/200 Batch 2/3 Cost: 14.323809\n",
      "Epoch   23/200 Batch 3/3 Cost: 1.879480\n",
      "Epoch   24/200 Batch 1/3 Cost: 4.528992\n",
      "Epoch   24/200 Batch 2/3 Cost: 11.054780\n",
      "Epoch   24/200 Batch 3/3 Cost: 18.489370\n",
      "Epoch   25/200 Batch 1/3 Cost: 6.455124\n",
      "Epoch   25/200 Batch 2/3 Cost: 8.164529\n",
      "Epoch   25/200 Batch 3/3 Cost: 18.886595\n",
      "Epoch   26/200 Batch 1/3 Cost: 4.122293\n",
      "Epoch   26/200 Batch 2/3 Cost: 24.715130\n",
      "Epoch   26/200 Batch 3/3 Cost: 5.749355\n",
      "Epoch   27/200 Batch 1/3 Cost: 16.812265\n",
      "Epoch   27/200 Batch 2/3 Cost: 8.005900\n",
      "Epoch   27/200 Batch 3/3 Cost: 11.200163\n",
      "Epoch   28/200 Batch 1/3 Cost: 2.404252\n",
      "Epoch   28/200 Batch 2/3 Cost: 14.556049\n",
      "Epoch   28/200 Batch 3/3 Cost: 19.358335\n",
      "Epoch   29/200 Batch 1/3 Cost: 12.129132\n",
      "Epoch   29/200 Batch 2/3 Cost: 8.138347\n",
      "Epoch   29/200 Batch 3/3 Cost: 6.630452\n",
      "Epoch   30/200 Batch 1/3 Cost: 11.603466\n",
      "Epoch   30/200 Batch 2/3 Cost: 1.880555\n",
      "Epoch   30/200 Batch 3/3 Cost: 21.744656\n",
      "Epoch   31/200 Batch 1/3 Cost: 14.525860\n",
      "Epoch   31/200 Batch 2/3 Cost: 5.301771\n",
      "Epoch   31/200 Batch 3/3 Cost: 6.219063\n",
      "Epoch   32/200 Batch 1/3 Cost: 7.120408\n",
      "Epoch   32/200 Batch 2/3 Cost: 9.249752\n",
      "Epoch   32/200 Batch 3/3 Cost: 15.119158\n",
      "Epoch   33/200 Batch 1/3 Cost: 9.676702\n",
      "Epoch   33/200 Batch 2/3 Cost: 6.095132\n",
      "Epoch   33/200 Batch 3/3 Cost: 24.015472\n",
      "Epoch   34/200 Batch 1/3 Cost: 11.407269\n",
      "Epoch   34/200 Batch 2/3 Cost: 4.079473\n",
      "Epoch   34/200 Batch 3/3 Cost: 19.511976\n",
      "Epoch   35/200 Batch 1/3 Cost: 11.925351\n",
      "Epoch   35/200 Batch 2/3 Cost: 8.911746\n",
      "Epoch   35/200 Batch 3/3 Cost: 4.571307\n",
      "Epoch   36/200 Batch 1/3 Cost: 11.417927\n",
      "Epoch   36/200 Batch 2/3 Cost: 9.027523\n",
      "Epoch   36/200 Batch 3/3 Cost: 4.280194\n",
      "Epoch   37/200 Batch 1/3 Cost: 14.370624\n",
      "Epoch   37/200 Batch 2/3 Cost: 1.217842\n",
      "Epoch   37/200 Batch 3/3 Cost: 17.500414\n",
      "Epoch   38/200 Batch 1/3 Cost: 7.081102\n",
      "Epoch   38/200 Batch 2/3 Cost: 21.951796\n",
      "Epoch   38/200 Batch 3/3 Cost: 2.148584\n",
      "Epoch   39/200 Batch 1/3 Cost: 14.270996\n",
      "Epoch   39/200 Batch 2/3 Cost: 1.293505\n",
      "Epoch   39/200 Batch 3/3 Cost: 17.313375\n",
      "Epoch   40/200 Batch 1/3 Cost: 8.899023\n",
      "Epoch   40/200 Batch 2/3 Cost: 11.653796\n",
      "Epoch   40/200 Batch 3/3 Cost: 4.051802\n",
      "Epoch   41/200 Batch 1/3 Cost: 1.240807\n",
      "Epoch   41/200 Batch 2/3 Cost: 14.620799\n",
      "Epoch   41/200 Batch 3/3 Cost: 14.581034\n",
      "Epoch   42/200 Batch 1/3 Cost: 13.839142\n",
      "Epoch   42/200 Batch 2/3 Cost: 9.976245\n",
      "Epoch   42/200 Batch 3/3 Cost: 9.553736\n",
      "Epoch   43/200 Batch 1/3 Cost: 4.857656\n",
      "Epoch   43/200 Batch 2/3 Cost: 24.549347\n",
      "Epoch   43/200 Batch 3/3 Cost: 2.773016\n",
      "Epoch   44/200 Batch 1/3 Cost: 14.077655\n",
      "Epoch   44/200 Batch 2/3 Cost: 1.414688\n",
      "Epoch   44/200 Batch 3/3 Cost: 16.977125\n",
      "Epoch   45/200 Batch 1/3 Cost: 11.730781\n",
      "Epoch   45/200 Batch 2/3 Cost: 7.944212\n",
      "Epoch   45/200 Batch 3/3 Cost: 6.446683\n",
      "Epoch   46/200 Batch 1/3 Cost: 6.880476\n",
      "Epoch   46/200 Batch 2/3 Cost: 8.613420\n",
      "Epoch   46/200 Batch 3/3 Cost: 22.234932\n",
      "Epoch   47/200 Batch 1/3 Cost: 6.001236\n",
      "Epoch   47/200 Batch 2/3 Cost: 7.906286\n",
      "Epoch   47/200 Batch 3/3 Cost: 17.151600\n",
      "Epoch   48/200 Batch 1/3 Cost: 14.321756\n",
      "Epoch   48/200 Batch 2/3 Cost: 0.930603\n",
      "Epoch   48/200 Batch 3/3 Cost: 17.742277\n",
      "Epoch   49/200 Batch 1/3 Cost: 6.860934\n",
      "Epoch   49/200 Batch 2/3 Cost: 14.549431\n",
      "Epoch   49/200 Batch 3/3 Cost: 20.987600\n",
      "Epoch   50/200 Batch 1/3 Cost: 10.150273\n",
      "Epoch   50/200 Batch 2/3 Cost: 6.730843\n",
      "Epoch   50/200 Batch 3/3 Cost: 17.867828\n",
      "Epoch   51/200 Batch 1/3 Cost: 5.173996\n",
      "Epoch   51/200 Batch 2/3 Cost: 12.320845\n",
      "Epoch   51/200 Batch 3/3 Cost: 19.247318\n",
      "Epoch   52/200 Batch 1/3 Cost: 6.997545\n",
      "Epoch   52/200 Batch 2/3 Cost: 20.417704\n",
      "Epoch   52/200 Batch 3/3 Cost: 3.406000\n",
      "Epoch   53/200 Batch 1/3 Cost: 11.613340\n",
      "Epoch   53/200 Batch 2/3 Cost: 9.271054\n",
      "Epoch   53/200 Batch 3/3 Cost: 3.653887\n",
      "Epoch   54/200 Batch 1/3 Cost: 4.007140\n",
      "Epoch   54/200 Batch 2/3 Cost: 10.721598\n",
      "Epoch   54/200 Batch 3/3 Cost: 17.733408\n",
      "Epoch   55/200 Batch 1/3 Cost: 13.254600\n",
      "Epoch   55/200 Batch 2/3 Cost: 15.208794\n",
      "Epoch   55/200 Batch 3/3 Cost: 9.018778\n",
      "Epoch   56/200 Batch 1/3 Cost: 6.751454\n",
      "Epoch   56/200 Batch 2/3 Cost: 8.135545\n",
      "Epoch   56/200 Batch 3/3 Cost: 14.966825\n",
      "Epoch   57/200 Batch 1/3 Cost: 11.371692\n",
      "Epoch   57/200 Batch 2/3 Cost: 5.359047\n",
      "Epoch   57/200 Batch 3/3 Cost: 19.166937\n",
      "Epoch   58/200 Batch 1/3 Cost: 5.085023\n",
      "Epoch   58/200 Batch 2/3 Cost: 13.790290\n",
      "Epoch   58/200 Batch 3/3 Cost: 5.530277\n",
      "Epoch   59/200 Batch 1/3 Cost: 15.973186\n",
      "Epoch   59/200 Batch 2/3 Cost: 13.997242\n",
      "Epoch   59/200 Batch 3/3 Cost: 0.704774\n",
      "Epoch   60/200 Batch 1/3 Cost: 18.488163\n",
      "Epoch   60/200 Batch 2/3 Cost: 6.477263\n",
      "Epoch   60/200 Batch 3/3 Cost: 9.889324\n",
      "Epoch   61/200 Batch 1/3 Cost: 24.416817\n",
      "Epoch   61/200 Batch 2/3 Cost: 4.354964\n",
      "Epoch   61/200 Batch 3/3 Cost: 8.800616\n",
      "Epoch   62/200 Batch 1/3 Cost: 1.875771\n",
      "Epoch   62/200 Batch 2/3 Cost: 15.826542\n",
      "Epoch   62/200 Batch 3/3 Cost: 15.621889\n",
      "Epoch   63/200 Batch 1/3 Cost: 7.734083\n",
      "Epoch   63/200 Batch 2/3 Cost: 11.237393\n",
      "Epoch   63/200 Batch 3/3 Cost: 5.726913\n",
      "Epoch   64/200 Batch 1/3 Cost: 10.982553\n",
      "Epoch   64/200 Batch 2/3 Cost: 1.715497\n",
      "Epoch   64/200 Batch 3/3 Cost: 20.692671\n",
      "Epoch   65/200 Batch 1/3 Cost: 10.964109\n",
      "Epoch   65/200 Batch 2/3 Cost: 7.748535\n",
      "Epoch   65/200 Batch 3/3 Cost: 5.901676\n",
      "Epoch   66/200 Batch 1/3 Cost: 7.254389\n",
      "Epoch   66/200 Batch 2/3 Cost: 13.223792\n",
      "Epoch   66/200 Batch 3/3 Cost: 2.003070\n",
      "Epoch   67/200 Batch 1/3 Cost: 0.766245\n",
      "Epoch   67/200 Batch 2/3 Cost: 19.809864\n",
      "Epoch   67/200 Batch 3/3 Cost: 16.814554\n",
      "Epoch   68/200 Batch 1/3 Cost: 3.983589\n",
      "Epoch   68/200 Batch 2/3 Cost: 24.200764\n",
      "Epoch   68/200 Batch 3/3 Cost: 3.512193\n",
      "Epoch   69/200 Batch 1/3 Cost: 9.231663\n",
      "Epoch   69/200 Batch 2/3 Cost: 6.147634\n",
      "Epoch   69/200 Batch 3/3 Cost: 13.827399\n",
      "Epoch   70/200 Batch 1/3 Cost: 3.130216\n",
      "Epoch   70/200 Batch 2/3 Cost: 13.844115\n",
      "Epoch   70/200 Batch 3/3 Cost: 13.883288\n",
      "Epoch   71/200 Batch 1/3 Cost: 5.182439\n",
      "Epoch   71/200 Batch 2/3 Cost: 13.607544\n",
      "Epoch   71/200 Batch 3/3 Cost: 5.890338\n",
      "Epoch   72/200 Batch 1/3 Cost: 6.709512\n",
      "Epoch   72/200 Batch 2/3 Cost: 13.087070\n",
      "Epoch   72/200 Batch 3/3 Cost: 3.582718\n",
      "Epoch   73/200 Batch 1/3 Cost: 3.846397\n",
      "Epoch   73/200 Batch 2/3 Cost: 22.677069\n",
      "Epoch   73/200 Batch 3/3 Cost: 5.476654\n",
      "Epoch   74/200 Batch 1/3 Cost: 13.300106\n",
      "Epoch   74/200 Batch 2/3 Cost: 1.432988\n",
      "Epoch   74/200 Batch 3/3 Cost: 16.118996\n",
      "Epoch   75/200 Batch 1/3 Cost: 6.919262\n",
      "Epoch   75/200 Batch 2/3 Cost: 13.801678\n",
      "Epoch   75/200 Batch 3/3 Cost: 20.042208\n",
      "Epoch   76/200 Batch 1/3 Cost: 9.697309\n",
      "Epoch   76/200 Batch 2/3 Cost: 5.609808\n",
      "Epoch   76/200 Batch 3/3 Cost: 13.702062\n",
      "Epoch   77/200 Batch 1/3 Cost: 11.009216\n",
      "Epoch   77/200 Batch 2/3 Cost: 6.012819\n",
      "Epoch   77/200 Batch 3/3 Cost: 18.907953\n",
      "Epoch   78/200 Batch 1/3 Cost: 11.830244\n",
      "Epoch   78/200 Batch 2/3 Cost: 6.947560\n",
      "Epoch   78/200 Batch 3/3 Cost: 13.807434\n",
      "Epoch   79/200 Batch 1/3 Cost: 5.571089\n",
      "Epoch   79/200 Batch 2/3 Cost: 13.278252\n",
      "Epoch   79/200 Batch 3/3 Cost: 17.390411\n",
      "Epoch   80/200 Batch 1/3 Cost: 7.535346\n",
      "Epoch   80/200 Batch 2/3 Cost: 10.884140\n",
      "Epoch   80/200 Batch 3/3 Cost: 5.629758\n",
      "Epoch   81/200 Batch 1/3 Cost: 15.232601\n",
      "Epoch   81/200 Batch 2/3 Cost: 12.680708\n",
      "Epoch   81/200 Batch 3/3 Cost: 1.747136\n",
      "Epoch   82/200 Batch 1/3 Cost: 19.156796\n",
      "Epoch   82/200 Batch 2/3 Cost: 11.482174\n",
      "Epoch   82/200 Batch 3/3 Cost: 0.402869\n",
      "Epoch   83/200 Batch 1/3 Cost: 0.335391\n",
      "Epoch   83/200 Batch 2/3 Cost: 20.424252\n",
      "Epoch   83/200 Batch 3/3 Cost: 15.686125\n",
      "Epoch   84/200 Batch 1/3 Cost: 11.153018\n",
      "Epoch   84/200 Batch 2/3 Cost: 1.156530\n",
      "Epoch   84/200 Batch 3/3 Cost: 20.733921\n",
      "Epoch   85/200 Batch 1/3 Cost: 10.057684\n",
      "Epoch   85/200 Batch 2/3 Cost: 7.392653\n",
      "Epoch   85/200 Batch 3/3 Cost: 14.334686\n",
      "Epoch   86/200 Batch 1/3 Cost: 14.432344\n",
      "Epoch   86/200 Batch 2/3 Cost: 9.036670\n",
      "Epoch   86/200 Batch 3/3 Cost: 8.416869\n",
      "Epoch   87/200 Batch 1/3 Cost: 15.357418\n",
      "Epoch   87/200 Batch 2/3 Cost: 8.908069\n",
      "Epoch   87/200 Batch 3/3 Cost: 8.144467\n",
      "Epoch   88/200 Batch 1/3 Cost: 7.871757\n",
      "Epoch   88/200 Batch 2/3 Cost: 14.114196\n",
      "Epoch   88/200 Batch 3/3 Cost: 11.574074\n",
      "Epoch   89/200 Batch 1/3 Cost: 5.258866\n",
      "Epoch   89/200 Batch 2/3 Cost: 14.977818\n",
      "Epoch   89/200 Batch 3/3 Cost: 12.720016\n",
      "Epoch   90/200 Batch 1/3 Cost: 11.612041\n",
      "Epoch   90/200 Batch 2/3 Cost: 9.227815\n",
      "Epoch   90/200 Batch 3/3 Cost: 3.036440\n",
      "Epoch   91/200 Batch 1/3 Cost: 9.827873\n",
      "Epoch   91/200 Batch 2/3 Cost: 6.516232\n",
      "Epoch   91/200 Batch 3/3 Cost: 17.661642\n",
      "Epoch   92/200 Batch 1/3 Cost: 6.894038\n",
      "Epoch   92/200 Batch 2/3 Cost: 13.042340\n",
      "Epoch   92/200 Batch 3/3 Cost: 11.972597\n",
      "Epoch   93/200 Batch 1/3 Cost: 9.287331\n",
      "Epoch   93/200 Batch 2/3 Cost: 5.132629\n",
      "Epoch   93/200 Batch 3/3 Cost: 21.946772\n",
      "Epoch   94/200 Batch 1/3 Cost: 4.827224\n",
      "Epoch   94/200 Batch 2/3 Cost: 12.747077\n",
      "Epoch   94/200 Batch 3/3 Cost: 4.849727\n",
      "Epoch   95/200 Batch 1/3 Cost: 9.526355\n",
      "Epoch   95/200 Batch 2/3 Cost: 6.008507\n",
      "Epoch   95/200 Batch 3/3 Cost: 16.732939\n",
      "Epoch   96/200 Batch 1/3 Cost: 6.631933\n",
      "Epoch   96/200 Batch 2/3 Cost: 13.184958\n",
      "Epoch   96/200 Batch 3/3 Cost: 11.946951\n",
      "Epoch   97/200 Batch 1/3 Cost: 11.191142\n",
      "Epoch   97/200 Batch 2/3 Cost: 4.531385\n",
      "Epoch   97/200 Batch 3/3 Cost: 18.476513\n",
      "Epoch   98/200 Batch 1/3 Cost: 10.518518\n",
      "Epoch   98/200 Batch 2/3 Cost: 15.153806\n",
      "Epoch   98/200 Batch 3/3 Cost: 7.971300\n",
      "Epoch   99/200 Batch 1/3 Cost: 5.874057\n",
      "Epoch   99/200 Batch 2/3 Cost: 12.437037\n",
      "Epoch   99/200 Batch 3/3 Cost: 3.734125\n",
      "Epoch  100/200 Batch 1/3 Cost: 3.712348\n",
      "Epoch  100/200 Batch 2/3 Cost: 9.865110\n",
      "Epoch  100/200 Batch 3/3 Cost: 16.354235\n",
      "Epoch  101/200 Batch 1/3 Cost: 10.695049\n",
      "Epoch  101/200 Batch 2/3 Cost: 8.024256\n",
      "Epoch  101/200 Batch 3/3 Cost: 4.371274\n",
      "Epoch  102/200 Batch 1/3 Cost: 1.281403\n",
      "Epoch  102/200 Batch 2/3 Cost: 17.770275\n",
      "Epoch  102/200 Batch 3/3 Cost: 16.135662\n",
      "Epoch  103/200 Batch 1/3 Cost: 12.823585\n",
      "Epoch  103/200 Batch 2/3 Cost: 1.026483\n",
      "Epoch  103/200 Batch 3/3 Cost: 16.009523\n",
      "Epoch  104/200 Batch 1/3 Cost: 6.426323\n",
      "Epoch  104/200 Batch 2/3 Cost: 9.990368\n",
      "Epoch  104/200 Batch 3/3 Cost: 14.523292\n",
      "Epoch  105/200 Batch 1/3 Cost: 4.992148\n",
      "Epoch  105/200 Batch 2/3 Cost: 21.115086\n",
      "Epoch  105/200 Batch 3/3 Cost: 2.367070\n",
      "Epoch  106/200 Batch 1/3 Cost: 0.836648\n",
      "Epoch  106/200 Batch 2/3 Cost: 13.101213\n",
      "Epoch  106/200 Batch 3/3 Cost: 13.485692\n",
      "Epoch  107/200 Batch 1/3 Cost: 4.888230\n",
      "Epoch  107/200 Batch 2/3 Cost: 11.595827\n",
      "Epoch  107/200 Batch 3/3 Cost: 17.176634\n",
      "Epoch  108/200 Batch 1/3 Cost: 10.368500\n",
      "Epoch  108/200 Batch 2/3 Cost: 7.167666\n",
      "Epoch  108/200 Batch 3/3 Cost: 5.910724\n",
      "Epoch  109/200 Batch 1/3 Cost: 10.051485\n",
      "Epoch  109/200 Batch 2/3 Cost: 8.612196\n",
      "Epoch  109/200 Batch 3/3 Cost: 2.167953\n",
      "Epoch  110/200 Batch 1/3 Cost: 8.967877\n",
      "Epoch  110/200 Batch 2/3 Cost: 5.880805\n",
      "Epoch  110/200 Batch 3/3 Cost: 12.422140\n",
      "Epoch  111/200 Batch 1/3 Cost: 10.481695\n",
      "Epoch  111/200 Batch 2/3 Cost: 5.548971\n",
      "Epoch  111/200 Batch 3/3 Cost: 17.959406\n",
      "Epoch  112/200 Batch 1/3 Cost: 12.493186\n",
      "Epoch  112/200 Batch 2/3 Cost: 5.445472\n",
      "Epoch  112/200 Batch 3/3 Cost: 3.899504\n",
      "Epoch  113/200 Batch 1/3 Cost: 8.457279\n",
      "Epoch  113/200 Batch 2/3 Cost: 7.239648\n",
      "Epoch  113/200 Batch 3/3 Cost: 15.965107\n",
      "Epoch  114/200 Batch 1/3 Cost: 5.533813\n",
      "Epoch  114/200 Batch 2/3 Cost: 10.028019\n",
      "Epoch  114/200 Batch 3/3 Cost: 17.148693\n",
      "Epoch  115/200 Batch 1/3 Cost: 11.065130\n",
      "Epoch  115/200 Batch 2/3 Cost: 9.002126\n",
      "Epoch  115/200 Batch 3/3 Cost: 8.841041\n",
      "Epoch  116/200 Batch 1/3 Cost: 6.088565\n",
      "Epoch  116/200 Batch 2/3 Cost: 11.479299\n",
      "Epoch  116/200 Batch 3/3 Cost: 16.223555\n",
      "Epoch  117/200 Batch 1/3 Cost: 4.777152\n",
      "Epoch  117/200 Batch 2/3 Cost: 7.849173\n",
      "Epoch  117/200 Batch 3/3 Cost: 15.384575\n",
      "Epoch  118/200 Batch 1/3 Cost: 16.274090\n",
      "Epoch  118/200 Batch 2/3 Cost: 11.593763\n",
      "Epoch  118/200 Batch 3/3 Cost: 0.519589\n",
      "Epoch  119/200 Batch 1/3 Cost: 7.541305\n",
      "Epoch  119/200 Batch 2/3 Cost: 11.968641\n",
      "Epoch  119/200 Batch 3/3 Cost: 2.921149\n",
      "Epoch  120/200 Batch 1/3 Cost: 7.239204\n",
      "Epoch  120/200 Batch 2/3 Cost: 11.944496\n",
      "Epoch  120/200 Batch 3/3 Cost: 3.008692\n",
      "Epoch  121/200 Batch 1/3 Cost: 9.641383\n",
      "Epoch  121/200 Batch 2/3 Cost: 5.570007\n",
      "Epoch  121/200 Batch 3/3 Cost: 11.962357\n",
      "Epoch  122/200 Batch 1/3 Cost: 18.732555\n",
      "Epoch  122/200 Batch 2/3 Cost: 9.518418\n",
      "Epoch  122/200 Batch 3/3 Cost: 1.190621\n",
      "Epoch  123/200 Batch 1/3 Cost: 2.693749\n",
      "Epoch  123/200 Batch 2/3 Cost: 23.613846\n",
      "Epoch  123/200 Batch 3/3 Cost: 2.851984\n",
      "Epoch  124/200 Batch 1/3 Cost: 8.976886\n",
      "Epoch  124/200 Batch 2/3 Cost: 5.225331\n",
      "Epoch  124/200 Batch 3/3 Cost: 12.351789\n",
      "Epoch  125/200 Batch 1/3 Cost: 8.139825\n",
      "Epoch  125/200 Batch 2/3 Cost: 8.277988\n",
      "Epoch  125/200 Batch 3/3 Cost: 11.413914\n",
      "Epoch  126/200 Batch 1/3 Cost: 10.448067\n",
      "Epoch  126/200 Batch 2/3 Cost: 5.236624\n",
      "Epoch  126/200 Batch 3/3 Cost: 17.710796\n",
      "Epoch  127/200 Batch 1/3 Cost: 7.718757\n",
      "Epoch  127/200 Batch 2/3 Cost: 4.571660\n",
      "Epoch  127/200 Batch 3/3 Cost: 14.614847\n",
      "Epoch  128/200 Batch 1/3 Cost: 16.316574\n",
      "Epoch  128/200 Batch 2/3 Cost: 11.229117\n",
      "Epoch  128/200 Batch 3/3 Cost: 0.493441\n",
      "Epoch  129/200 Batch 1/3 Cost: 9.771362\n",
      "Epoch  129/200 Batch 2/3 Cost: 5.635544\n",
      "Epoch  129/200 Batch 3/3 Cost: 11.582485\n",
      "Epoch  130/200 Batch 1/3 Cost: 10.817179\n",
      "Epoch  130/200 Batch 2/3 Cost: 9.015962\n",
      "Epoch  130/200 Batch 3/3 Cost: 1.414109\n",
      "Epoch  131/200 Batch 1/3 Cost: 3.802148\n",
      "Epoch  131/200 Batch 2/3 Cost: 11.523555\n",
      "Epoch  131/200 Batch 3/3 Cost: 11.721241\n",
      "Epoch  132/200 Batch 1/3 Cost: 7.023899\n",
      "Epoch  132/200 Batch 2/3 Cost: 9.950360\n",
      "Epoch  132/200 Batch 3/3 Cost: 5.496240\n",
      "Epoch  133/200 Batch 1/3 Cost: 4.126394\n",
      "Epoch  133/200 Batch 2/3 Cost: 19.380264\n",
      "Epoch  133/200 Batch 3/3 Cost: 5.450760\n",
      "Epoch  134/200 Batch 1/3 Cost: 11.866653\n",
      "Epoch  134/200 Batch 2/3 Cost: 5.641092\n",
      "Epoch  134/200 Batch 3/3 Cost: 3.716572\n",
      "Epoch  135/200 Batch 1/3 Cost: 7.046363\n",
      "Epoch  135/200 Batch 2/3 Cost: 7.475412\n",
      "Epoch  135/200 Batch 3/3 Cost: 11.842857\n",
      "Epoch  136/200 Batch 1/3 Cost: 10.610991\n",
      "Epoch  136/200 Batch 2/3 Cost: 8.473727\n",
      "Epoch  136/200 Batch 3/3 Cost: 2.922558\n",
      "Epoch  137/200 Batch 1/3 Cost: 0.467107\n",
      "Epoch  137/200 Batch 2/3 Cost: 18.325893\n",
      "Epoch  137/200 Batch 3/3 Cost: 14.233073\n",
      "Epoch  138/200 Batch 1/3 Cost: 16.114258\n",
      "Epoch  138/200 Batch 2/3 Cost: 10.226788\n",
      "Epoch  138/200 Batch 3/3 Cost: 1.389359\n",
      "Epoch  139/200 Batch 1/3 Cost: 10.524425\n",
      "Epoch  139/200 Batch 2/3 Cost: 8.411509\n",
      "Epoch  139/200 Batch 3/3 Cost: 2.927359\n",
      "Epoch  140/200 Batch 1/3 Cost: 3.156364\n",
      "Epoch  140/200 Batch 2/3 Cost: 11.143394\n",
      "Epoch  140/200 Batch 3/3 Cost: 12.294367\n",
      "Epoch  141/200 Batch 1/3 Cost: 7.702491\n",
      "Epoch  141/200 Batch 2/3 Cost: 4.555543\n",
      "Epoch  141/200 Batch 3/3 Cost: 14.799058\n",
      "Epoch  142/200 Batch 1/3 Cost: 0.528979\n",
      "Epoch  142/200 Batch 2/3 Cost: 12.420950\n",
      "Epoch  142/200 Batch 3/3 Cost: 13.139961\n",
      "Epoch  143/200 Batch 1/3 Cost: 5.416807\n",
      "Epoch  143/200 Batch 2/3 Cost: 6.771404\n",
      "Epoch  143/200 Batch 3/3 Cost: 14.740417\n",
      "Epoch  144/200 Batch 1/3 Cost: 9.194907\n",
      "Epoch  144/200 Batch 2/3 Cost: 5.013993\n",
      "Epoch  144/200 Batch 3/3 Cost: 15.948040\n",
      "Epoch  145/200 Batch 1/3 Cost: 5.784929\n",
      "Epoch  145/200 Batch 2/3 Cost: 9.295864\n",
      "Epoch  145/200 Batch 3/3 Cost: 13.639213\n",
      "Epoch  146/200 Batch 1/3 Cost: 5.614741\n",
      "Epoch  146/200 Batch 2/3 Cost: 6.757728\n",
      "Epoch  146/200 Batch 3/3 Cost: 15.014442\n",
      "Epoch  147/200 Batch 1/3 Cost: 6.642325\n",
      "Epoch  147/200 Batch 2/3 Cost: 11.356640\n",
      "Epoch  147/200 Batch 3/3 Cost: 3.019712\n",
      "Epoch  148/200 Batch 1/3 Cost: 0.488226\n",
      "Epoch  148/200 Batch 2/3 Cost: 12.307366\n",
      "Epoch  148/200 Batch 3/3 Cost: 13.075547\n",
      "Epoch  149/200 Batch 1/3 Cost: 9.721201\n",
      "Epoch  149/200 Batch 2/3 Cost: 4.795228\n",
      "Epoch  149/200 Batch 3/3 Cost: 15.327053\n",
      "Epoch  150/200 Batch 1/3 Cost: 9.683004\n",
      "Epoch  150/200 Batch 2/3 Cost: 7.437806\n",
      "Epoch  150/200 Batch 3/3 Cost: 4.045846\n",
      "Epoch  151/200 Batch 1/3 Cost: 9.247669\n",
      "Epoch  151/200 Batch 2/3 Cost: 7.452128\n",
      "Epoch  151/200 Batch 3/3 Cost: 3.691668\n",
      "Epoch  152/200 Batch 1/3 Cost: 3.717443\n",
      "Epoch  152/200 Batch 2/3 Cost: 11.077558\n",
      "Epoch  152/200 Batch 3/3 Cost: 11.301194\n",
      "Epoch  153/200 Batch 1/3 Cost: 4.959958\n",
      "Epoch  153/200 Batch 2/3 Cost: 18.987816\n",
      "Epoch  153/200 Batch 3/3 Cost: 2.137416\n",
      "Epoch  154/200 Batch 1/3 Cost: 8.337349\n",
      "Epoch  154/200 Batch 2/3 Cost: 8.959397\n",
      "Epoch  154/200 Batch 3/3 Cost: 3.015365\n",
      "Epoch  155/200 Batch 1/3 Cost: 0.488275\n",
      "Epoch  155/200 Batch 2/3 Cost: 10.328633\n",
      "Epoch  155/200 Batch 3/3 Cost: 16.075773\n",
      "Epoch  156/200 Batch 1/3 Cost: 5.858805\n",
      "Epoch  156/200 Batch 2/3 Cost: 12.265782\n",
      "Epoch  156/200 Batch 3/3 Cost: 10.563194\n",
      "Epoch  157/200 Batch 1/3 Cost: 8.299735\n",
      "Epoch  157/200 Batch 2/3 Cost: 4.544157\n",
      "Epoch  157/200 Batch 3/3 Cost: 19.675701\n",
      "Epoch  158/200 Batch 1/3 Cost: 8.050221\n",
      "Epoch  158/200 Batch 2/3 Cost: 6.785405\n",
      "Epoch  158/200 Batch 3/3 Cost: 12.910981\n",
      "Epoch  159/200 Batch 1/3 Cost: 5.589266\n",
      "Epoch  159/200 Batch 2/3 Cost: 10.319100\n",
      "Epoch  159/200 Batch 3/3 Cost: 15.252949\n",
      "Epoch  160/200 Batch 1/3 Cost: 5.486576\n",
      "Epoch  160/200 Batch 2/3 Cost: 18.039246\n",
      "Epoch  160/200 Batch 3/3 Cost: 1.952812\n",
      "Epoch  161/200 Batch 1/3 Cost: 0.720971\n",
      "Epoch  161/200 Batch 2/3 Cost: 11.916674\n",
      "Epoch  161/200 Batch 3/3 Cost: 12.450013\n",
      "Epoch  162/200 Batch 1/3 Cost: 9.524358\n",
      "Epoch  162/200 Batch 2/3 Cost: 6.570250\n",
      "Epoch  162/200 Batch 3/3 Cost: 5.683250\n",
      "Epoch  163/200 Batch 1/3 Cost: 5.728197\n",
      "Epoch  163/200 Batch 2/3 Cost: 7.505059\n",
      "Epoch  163/200 Batch 3/3 Cost: 11.497787\n",
      "Epoch  164/200 Batch 1/3 Cost: 10.008819\n",
      "Epoch  164/200 Batch 2/3 Cost: 8.429976\n",
      "Epoch  164/200 Batch 3/3 Cost: 1.443767\n",
      "Epoch  165/200 Batch 1/3 Cost: 3.576832\n",
      "Epoch  165/200 Batch 2/3 Cost: 9.309694\n",
      "Epoch  165/200 Batch 3/3 Cost: 14.201429\n",
      "Epoch  166/200 Batch 1/3 Cost: 6.643673\n",
      "Epoch  166/200 Batch 2/3 Cost: 9.296290\n",
      "Epoch  166/200 Batch 3/3 Cost: 5.220335\n",
      "Epoch  167/200 Batch 1/3 Cost: 5.817545\n",
      "Epoch  167/200 Batch 2/3 Cost: 10.953429\n",
      "Epoch  167/200 Batch 3/3 Cost: 3.201618\n",
      "Epoch  168/200 Batch 1/3 Cost: 11.409251\n",
      "Epoch  168/200 Batch 2/3 Cost: 5.765793\n",
      "Epoch  168/200 Batch 3/3 Cost: 3.263730\n",
      "Epoch  169/200 Batch 1/3 Cost: 3.115913\n",
      "Epoch  169/200 Batch 2/3 Cost: 10.522001\n",
      "Epoch  169/200 Batch 3/3 Cost: 11.630415\n",
      "Epoch  170/200 Batch 1/3 Cost: 11.566982\n",
      "Epoch  170/200 Batch 2/3 Cost: 5.052751\n",
      "Epoch  170/200 Batch 3/3 Cost: 4.082205\n",
      "Epoch  171/200 Batch 1/3 Cost: 4.601083\n",
      "Epoch  171/200 Batch 2/3 Cost: 10.076064\n",
      "Epoch  171/200 Batch 3/3 Cost: 10.320411\n",
      "Epoch  172/200 Batch 1/3 Cost: 11.629488\n",
      "Epoch  172/200 Batch 2/3 Cost: 7.969825\n",
      "Epoch  172/200 Batch 3/3 Cost: 7.596527\n",
      "Epoch  173/200 Batch 1/3 Cost: 4.909879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  173/200 Batch 2/3 Cost: 11.391127\n",
      "Epoch  173/200 Batch 3/3 Cost: 5.688272\n",
      "Epoch  174/200 Batch 1/3 Cost: 1.065733\n",
      "Epoch  174/200 Batch 2/3 Cost: 11.542406\n",
      "Epoch  174/200 Batch 3/3 Cost: 11.795015\n",
      "Epoch  175/200 Batch 1/3 Cost: 10.879967\n",
      "Epoch  175/200 Batch 2/3 Cost: 5.932932\n",
      "Epoch  175/200 Batch 3/3 Cost: 11.746853\n",
      "Epoch  176/200 Batch 1/3 Cost: 5.626665\n",
      "Epoch  176/200 Batch 2/3 Cost: 6.488648\n",
      "Epoch  176/200 Batch 3/3 Cost: 14.439792\n",
      "Epoch  177/200 Batch 1/3 Cost: 14.178154\n",
      "Epoch  177/200 Batch 2/3 Cost: 10.777699\n",
      "Epoch  177/200 Batch 3/3 Cost: 0.555018\n",
      "Epoch  178/200 Batch 1/3 Cost: 11.366336\n",
      "Epoch  178/200 Batch 2/3 Cost: 6.295934\n",
      "Epoch  178/200 Batch 3/3 Cost: 1.611516\n",
      "Epoch  179/200 Batch 1/3 Cost: 11.116509\n",
      "Epoch  179/200 Batch 2/3 Cost: 1.085520\n",
      "Epoch  179/200 Batch 3/3 Cost: 14.078641\n",
      "Epoch  180/200 Batch 1/3 Cost: 9.830754\n",
      "Epoch  180/200 Batch 2/3 Cost: 12.766415\n",
      "Epoch  180/200 Batch 3/3 Cost: 7.406544\n",
      "Epoch  181/200 Batch 1/3 Cost: 5.660856\n",
      "Epoch  181/200 Batch 2/3 Cost: 6.688429\n",
      "Epoch  181/200 Batch 3/3 Cost: 11.448893\n",
      "Epoch  182/200 Batch 1/3 Cost: 11.431298\n",
      "Epoch  182/200 Batch 2/3 Cost: 6.433209\n",
      "Epoch  182/200 Batch 3/3 Cost: 1.503034\n",
      "Epoch  183/200 Batch 1/3 Cost: 9.098220\n",
      "Epoch  183/200 Batch 2/3 Cost: 1.292198\n",
      "Epoch  183/200 Batch 3/3 Cost: 17.370564\n",
      "Epoch  184/200 Batch 1/3 Cost: 4.214133\n",
      "Epoch  184/200 Batch 2/3 Cost: 10.834269\n",
      "Epoch  184/200 Batch 3/3 Cost: 4.580773\n",
      "Epoch  185/200 Batch 1/3 Cost: 0.822477\n",
      "Epoch  185/200 Batch 2/3 Cost: 16.227226\n",
      "Epoch  185/200 Batch 3/3 Cost: 13.325797\n",
      "Epoch  186/200 Batch 1/3 Cost: 14.553883\n",
      "Epoch  186/200 Batch 2/3 Cost: 9.562461\n",
      "Epoch  186/200 Batch 3/3 Cost: 1.375617\n",
      "Epoch  187/200 Batch 1/3 Cost: 7.588190\n",
      "Epoch  187/200 Batch 2/3 Cost: 10.642436\n",
      "Epoch  187/200 Batch 3/3 Cost: 1.239905\n",
      "Epoch  188/200 Batch 1/3 Cost: 6.616185\n",
      "Epoch  188/200 Batch 2/3 Cost: 10.569159\n",
      "Epoch  188/200 Batch 3/3 Cost: 1.524586\n",
      "Epoch  189/200 Batch 1/3 Cost: 13.958786\n",
      "Epoch  189/200 Batch 2/3 Cost: 5.900692\n",
      "Epoch  189/200 Batch 3/3 Cost: 7.623298\n",
      "Epoch  190/200 Batch 1/3 Cost: 19.387079\n",
      "Epoch  190/200 Batch 2/3 Cost: 3.815039\n",
      "Epoch  190/200 Batch 3/3 Cost: 6.638706\n",
      "Epoch  191/200 Batch 1/3 Cost: 11.015257\n",
      "Epoch  191/200 Batch 2/3 Cost: 6.207637\n",
      "Epoch  191/200 Batch 3/3 Cost: 9.008425\n",
      "Epoch  192/200 Batch 1/3 Cost: 9.812890\n",
      "Epoch  192/200 Batch 2/3 Cost: 5.651255\n",
      "Epoch  192/200 Batch 3/3 Cost: 9.619596\n",
      "Epoch  193/200 Batch 1/3 Cost: 17.215364\n",
      "Epoch  193/200 Batch 2/3 Cost: 4.492647\n",
      "Epoch  193/200 Batch 3/3 Cost: 6.925275\n",
      "Epoch  194/200 Batch 1/3 Cost: 10.747130\n",
      "Epoch  194/200 Batch 2/3 Cost: 8.725163\n",
      "Epoch  194/200 Batch 3/3 Cost: 1.958018\n",
      "Epoch  195/200 Batch 1/3 Cost: 9.313389\n",
      "Epoch  195/200 Batch 2/3 Cost: 0.974213\n",
      "Epoch  195/200 Batch 3/3 Cost: 17.456396\n",
      "Epoch  196/200 Batch 1/3 Cost: 4.838712\n",
      "Epoch  196/200 Batch 2/3 Cost: 7.923721\n",
      "Epoch  196/200 Batch 3/3 Cost: 15.383857\n",
      "Epoch  197/200 Batch 1/3 Cost: 6.967877\n",
      "Epoch  197/200 Batch 2/3 Cost: 4.146539\n",
      "Epoch  197/200 Batch 3/3 Cost: 12.820565\n",
      "Epoch  198/200 Batch 1/3 Cost: 0.470870\n",
      "Epoch  198/200 Batch 2/3 Cost: 16.615026\n",
      "Epoch  198/200 Batch 3/3 Cost: 12.579565\n",
      "Epoch  199/200 Batch 1/3 Cost: 14.480901\n",
      "Epoch  199/200 Batch 2/3 Cost: 5.466741\n",
      "Epoch  199/200 Batch 3/3 Cost: 7.282652\n",
      "Epoch  200/200 Batch 1/3 Cost: 8.898369\n",
      "Epoch  200/200 Batch 2/3 Cost: 10.494609\n",
      "Epoch  200/200 Batch 3/3 Cost: 0.913340\n",
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[154.3182]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서를 입력받아 Dataset의 형태로 변환해주는 라이브러리\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train,y_train)\n",
    "\n",
    "dataloader = DataLoader(dataset,batch_size=2,shuffle=True) # 통상적으로 미니 배치의 크기는 2의 배수 , 셔플은 True 로 한다.\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "\n",
    "nb_epochs = 200\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        #print(batch_idx)\n",
    "        #print(samples)\n",
    "        x_train,y_train = samples\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        cost = F.mse_loss(prediction,y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))\n",
    "\n",
    "# 임의의 입력값 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1ba44",
   "metadata": {},
   "source": [
    "## 7. 커스텀 데이터셋(Custom Dataset)\n",
    "#### 1. 커스텀 데이터셋(Custom Dataset)\n",
    "커스텀 데이터셋을 만들 때 기본적인 뼈대를 알아보자.\n",
    "~~~\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        #데이터셋의 전처리를 해주는 부분\n",
    "    def __len__(self):\n",
    "        #데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "    def __getitem__(self,idx):\n",
    "        #데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "812cdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(tensor([73., 80., 75.]), tensor([152.]))\n",
      "Epoch    0/20 Batch 1/3 Cost: 20278.720703\n",
      "Epoch    0/20 Batch 2/3 Cost: 7427.986328\n",
      "Epoch    0/20 Batch 3/3 Cost: 1561.607056\n",
      "Epoch    1/20 Batch 1/3 Cost: 647.595093\n",
      "Epoch    1/20 Batch 2/3 Cost: 242.359955\n",
      "Epoch    1/20 Batch 3/3 Cost: 79.836090\n",
      "Epoch    2/20 Batch 1/3 Cost: 26.838402\n",
      "Epoch    2/20 Batch 2/3 Cost: 3.010582\n",
      "Epoch    2/20 Batch 3/3 Cost: 2.581892\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.592217\n",
      "Epoch    3/20 Batch 2/3 Cost: 0.562889\n",
      "Epoch    3/20 Batch 3/3 Cost: 9.701928\n",
      "Epoch    4/20 Batch 1/3 Cost: 1.719886\n",
      "Epoch    4/20 Batch 2/3 Cost: 3.070674\n",
      "Epoch    4/20 Batch 3/3 Cost: 2.253847\n",
      "Epoch    5/20 Batch 1/3 Cost: 3.844807\n",
      "Epoch    5/20 Batch 2/3 Cost: 0.077713\n",
      "Epoch    5/20 Batch 3/3 Cost: 1.604550\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.157415\n",
      "Epoch    6/20 Batch 2/3 Cost: 0.790989\n",
      "Epoch    6/20 Batch 3/3 Cost: 9.128262\n",
      "Epoch    7/20 Batch 1/3 Cost: 0.551075\n",
      "Epoch    7/20 Batch 2/3 Cost: 1.745896\n",
      "Epoch    7/20 Batch 3/3 Cost: 7.641172\n",
      "Epoch    8/20 Batch 1/3 Cost: 1.882972\n",
      "Epoch    8/20 Batch 2/3 Cost: 2.806770\n",
      "Epoch    8/20 Batch 3/3 Cost: 2.788945\n",
      "Epoch    9/20 Batch 1/3 Cost: 3.949769\n",
      "Epoch    9/20 Batch 2/3 Cost: 0.720058\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.007182\n",
      "Epoch   10/20 Batch 1/3 Cost: 0.641952\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.207004\n",
      "Epoch   10/20 Batch 3/3 Cost: 8.155619\n",
      "Epoch   11/20 Batch 1/3 Cost: 3.068735\n",
      "Epoch   11/20 Batch 2/3 Cost: 3.136216\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.570839\n",
      "Epoch   12/20 Batch 1/3 Cost: 0.868101\n",
      "Epoch   12/20 Batch 2/3 Cost: 3.430048\n",
      "Epoch   12/20 Batch 3/3 Cost: 1.751010\n",
      "Epoch   13/20 Batch 1/3 Cost: 3.710308\n",
      "Epoch   13/20 Batch 2/3 Cost: 1.861773\n",
      "Epoch   13/20 Batch 3/3 Cost: 0.099390\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.015233\n",
      "Epoch   14/20 Batch 2/3 Cost: 1.010633\n",
      "Epoch   14/20 Batch 3/3 Cost: 8.576544\n",
      "Epoch   15/20 Batch 1/3 Cost: 1.735540\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.728471\n",
      "Epoch   15/20 Batch 3/3 Cost: 7.016242\n",
      "Epoch   16/20 Batch 1/3 Cost: 2.195718\n",
      "Epoch   16/20 Batch 2/3 Cost: 2.591258\n",
      "Epoch   16/20 Batch 3/3 Cost: 2.488553\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.630512\n",
      "Epoch   17/20 Batch 2/3 Cost: 0.202413\n",
      "Epoch   17/20 Batch 3/3 Cost: 8.073208\n",
      "Epoch   18/20 Batch 1/3 Cost: 1.924477\n",
      "Epoch   18/20 Batch 2/3 Cost: 2.953283\n",
      "Epoch   18/20 Batch 3/3 Cost: 2.707395\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.687093\n",
      "Epoch   19/20 Batch 2/3 Cost: 3.838907\n",
      "Epoch   19/20 Batch 3/3 Cost: 1.607064\n",
      "Epoch   20/20 Batch 1/3 Cost: 0.207491\n",
      "Epoch   20/20 Batch 2/3 Cost: 4.401663\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.001752\n",
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[149.5370]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "  def __init__(self):\n",
    "    self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "    self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "  def __getitem__(self, idx): \n",
    "    x = torch.FloatTensor(self.x_data[idx])\n",
    "    y = torch.FloatTensor(self.y_data[idx])\n",
    "    return x, y\n",
    "\n",
    "dataset = CustomDataset()\n",
    "print(len(dataset))\n",
    "print(dataset[0])\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "\n",
    "    x_train, y_train = samples\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))\n",
    "\n",
    "# 임의의 입력값을 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
