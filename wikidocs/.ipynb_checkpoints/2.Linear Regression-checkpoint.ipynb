{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfd02dc",
   "metadata": {},
   "source": [
    "# 선형 회귀 (Linear Regression)\n",
    "---\n",
    "\n",
    "\n",
    "## 1. 선형 회귀 (Linear Regression)\n",
    "### 1. 데이터에 대한 이해(Data Definition)\n",
    "1. 훈련 데이터셋과 테스트 데이터셋  \n",
    "훈련 데이터셋(training dataset) : 예측을 위해 사용하는 데이터 학습이 끝난 후, 모델이 얼마나 잘 작동하는지 판별하는 데이터셋\n",
    "2. 훈련 데이터셋의 구성  \n",
    "모델을 학습시키기 위해 텐서의 형태(torch.tensor)를 가지고 있는다.\n",
    "\n",
    "### 2. 가설(Hypothesis) 수립\n",
    "선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다.  \n",
    "이때 선형 회귀의 가설은 H(x) = Wx + b 와 같은 형식을 가지고 W를 가중치(Weight), b를 편향(bias)라고 한다.\n",
    "\n",
    "### 3. 비용 함수(Cost function)에 대한 이해\n",
    "비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)  \n",
    "평균 제곱 오차(MSE)는 회귀 문제에서 적절한 가중치와 편향을 찾기 위해서 최적화된 식이다.  \n",
    "평균 제곱 오차의 값을 최소값으로 만드는 가중치와 편향을 찾아내는 것이 가장 훈련 데이터를 잘 반영한 직선을 찾아내는 일이기 때문.\n",
    "\n",
    "### 4. 옵티마이저  - 경사 하강법(Gradient Descent)\n",
    "Cost function의 값을 최소로 하는 가중치와 편향을 찾을 때 옵티마이저(Optimizer) 알고리즘을 사용한다. 이때 사용하는 알고리즘 중 하나가 경사 하강법(Gradient Descent)이다.  \n",
    "이때 가중치의 값을 얼마나 변경할 지를 위한 적절한 학습률(learning rate)의 값을 찾아내는 것도 중요하다.  \n",
    "\n",
    "### 5. 파이토치로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f86e21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.1867, b: 0.0800, Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.7457, b: 0.5781, Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.8001, b: 0.4544, Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.8429, b: 0.3572, Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.8765, b: 0.2808, Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.9029, b: 0.2207, Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.9237, b: 0.1735, Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.9400, b: 0.1364, Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.9528, b: 0.1072, Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.9629, b: 0.0843, Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.9709, b: 0.0663, Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.9771, b: 0.0521, Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.9820, b: 0.0409, Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.9858, b: 0.0322, Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.9889, b: 0.0253, Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.9913, b: 0.0199, Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.9931, b: 0.0156, Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.9946, b: 0.0123, Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.9958, b: 0.0097, Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.9967, b: 0.0076, Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.9974, b: 0.0060, Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1) # 재실행해도 같은 결과가 나오도록 랜덤 시드를 준다.\n",
    "\n",
    "x_train = torch.FloatTensor([[1],[2],[3]]) # 입력\n",
    "y_train = torch.FloatTensor([[2],[4],[6]]) # 출력\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True) # 가중치 0으로 초기화, 학습을 통해 값이 변경되는 변수 명시\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b],lr=0.01) # optimizer 설정\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    hypothesis = x_train*W + b # 가설 선언\n",
    "\n",
    "    cost = torch.mean((hypothesis  - y_train)**2) # 비용 함수에 해당되는 평균 제곱 오차 선언\n",
    "\n",
    "\n",
    "    optimizer.zero_grad() # gradient 0으로 초기화\n",
    "    cost.backward() # 역전파 사용하여 gradient 계산\n",
    "    optimizer.step() # W와 b 업데이트\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.4f}, b: {:.4f}, Cost: {:.6f}'.format(epoch,nb_epochs,W.item(),b.item(),cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84515412",
   "metadata": {},
   "source": [
    "W는 2에 가깝고, b는 0에 가깝다. 현재 훈련데이터에서 실제 정답은 W=2, b=0이므로 거의 정답을 찾았다.  \n",
    "### 6. optimizer.zero_grad() 필요 이유\n",
    "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시킨다. 따라서 미분값을 0으로 계속 초기화시켜줘야 한다.\n",
    "### 7. torch.manual_seed()를 하는 이유\n",
    "torch.manual_seed()는 동일한 시드에서 난수 발생 순서와 값을 보장해준다.\n",
    "\n",
    "## 2. 자동 미분(Autograd)\n",
    "### 1. 자동 미분 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5826b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "y = w**2\n",
    "z = 2*y+5\n",
    "\n",
    "z.backward() #해당 수식의 w에 대한 기울기 계산\n",
    "\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad)) # w가 속한 수식을 w로 미분한 값이 w.grad에 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ef411",
   "metadata": {},
   "source": [
    "## 3. 다중 선형 회귀(Multivariable Linear regression)\n",
    "다중 선형 회귀 : 다수의 입력으로부터 출력을 예측.\n",
    "### 1. 파이토치로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f72d3a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b:0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b:0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b:0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b:0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b:0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b:0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b:0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b:0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b:0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b:0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b:0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73],[93],[89],[96],[73]])\n",
    "x2_train = torch.FloatTensor([[80],[88],[91],[98],[66]])\n",
    "x3_train = torch.FloatTensor([[75],[93],[90],[100],[70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 가중치와 편향 초기화\n",
    "w1 = torch.zeros(1,requires_grad=True)\n",
    "w2 = torch.zeros(1,requires_grad=True)\n",
    "w3 = torch.zeros(1,requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1,w2,w3,b],lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b:{:.3f} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,w1.item(),w2.item(),w3.item(),b.item(),cost.item()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd4062",
   "metadata": {},
   "source": [
    "### 2. 벡터와 행렬 연산으로 바꾸기\n",
    "입력의 개수가 많아 질수록 일일이 선언하기 비효율적이므로 행렬 곱셈 연산(또는 벡터의 내적)을 사용한다.\n",
    "\n",
    "### 3. 행렬 연산을 고려하여 파이토치로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a3ff73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b],lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    # b는 broadcasting되어 더해진다.\n",
    "    hypothesis = x_train.matmul(W)+b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item() # detach() : 기존 Tensor에서 gradient 전파가 안되는 텐서 생성\n",
    "    )) #hypothesis 가 점점 y_train값에 가까워진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01cdea",
   "metadata": {},
   "source": [
    "## 4. nn.Module로 구현하는 선형 회귀\n",
    "### 1. 단순 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2deaa6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n",
      "Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)\n",
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "# 모델을 선언 및 초기화 \n",
    "model = nn.Linear(1,1) # 단순 선형 회귀이므로 입력 뉴런과 출력 뉴런은 1\n",
    "\n",
    "print(list(model.parameters())) # 첫 번째 값: W 두 번째 값 : b\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction,y_train) #평균제곱오차사용\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "         print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n",
    "            \n",
    "# 최적화가 되었는지 확인\n",
    "\n",
    "new_var = torch.FloatTensor([[4.0]])\n",
    "pred_y = model(new_var)\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\",pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7bcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
